{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Librerías necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/clarabueno/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/clarabueno/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# 1. Para análisis de datos:\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# 2. Para visualizaciones\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# 3. Para procesamiento lingüístico con spaCy\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from spacy.tokens import Doc, Span, Token\n",
    "# 4. Para análisis de sentimientos y otras extensiones\n",
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('stopwords')\n",
    "# 5. Topic modeling (gensim)\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "# 6. BERTopic\n",
    "from bertopic import BERTopic\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# 7. Utilidades\n",
    "import re\n",
    "import glob\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Recolección del texto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Limpieza de .txt de Proyecto Gutenberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Limpieza básica de Gutenberg, reaprovechable para otros textos\n",
    "\n",
    "def limpiar_gutenberg(texto):\n",
    "    # Eliminar todo lo que aparece antes del marcador *** START OF\n",
    "    patron_inicio = r\"\\*\\*\\* START OF.*?\\*\\*\\*\"\n",
    "    patron_fin = r\"\\*\\*\\* END OF.*?\\*\\*\\*\"\n",
    "    \n",
    "    # Buscar el inicio del contenido real\n",
    "    inicio = re.search(patron_inicio, texto, flags=re.IGNORECASE | re.DOTALL)\n",
    "    if inicio:\n",
    "        texto = texto[inicio.end():]  # Cortar hasta el final del marcador START\n",
    "    \n",
    "    # Buscar el final del contenido real\n",
    "    fin = re.search(patron_fin, texto, flags=re.IGNORECASE | re.DOTALL)\n",
    "    if fin:\n",
    "        texto = texto[:fin.start()]  # Cortar antes del marcador END\n",
    "    \n",
    "    # Reducir saltos de línea excesivos (triple salto → doble salto)\n",
    "    texto = re.sub(r\"\\n\\s*\\n\\s*\\n+\", \"\\n\\n\", texto)\n",
    "    \n",
    "    return texto.strip()\n",
    "\n",
    "# 2. Limpieza + eliminar título/autora, reaprovechable con modificaciones\n",
    "\n",
    "def limpiar_gutenberg_y_metadatos(texto):\n",
    "    # Primero aplicamos la limpieza estándar\n",
    "    texto = limpiar_gutenberg(texto)\n",
    "\n",
    "    # Patrones que identifican líneas de encabezado o metadatos\n",
    "    patrones_eliminar = [\n",
    "        r\"\\blolly willowes\\b\",\n",
    "        r\"\\bby sylvia townsend warner\\b\",\n",
    "        r\"\\bsylvia townsend warner\\b\"\n",
    "    ]\n",
    "\n",
    "    lineas_limpias = []\n",
    "    for linea in texto.splitlines():\n",
    "        linea_sin_espacios = linea.strip()\n",
    "\n",
    "        # 1. Las líneas de encabezado suelen ser muy breves\n",
    "        linea_corta = len(linea_sin_espacios.split()) <= 6\n",
    "\n",
    "        # 2. Coinciden con alguno de los patrones de metadatos\n",
    "        es_metadato = any(\n",
    "            re.search(patron, linea_sin_espacios, flags=re.IGNORECASE)\n",
    "            for patron in patrones_eliminar\n",
    "        )\n",
    "\n",
    "        if linea_corta and es_metadato:\n",
    "            continue  # Eliminamos esa línea de encabezado\n",
    "        \n",
    "        lineas_limpias.append(linea)\n",
    "\n",
    "    texto = \"\\n\".join(lineas_limpias)\n",
    "\n",
    "    # Normalizar saltos de línea\n",
    "    texto = re.sub(r\"\\n\\s*\\n\\s*\\n+\", \"\\n\\n\", texto)\n",
    "    return texto.strip()\n",
    "\n",
    "# 3. Limpieza completa (reparación de guiones, OCR, metadatos)\n",
    "\n",
    "def limpiar_gutenberg_completo(texto):\n",
    "    # 1. Limpiar Gutenberg + encabezados/título/autora\n",
    "    texto = limpiar_gutenberg_y_metadatos(texto)\n",
    "\n",
    "    # 2. Reparar palabras cortadas con guión al final de la línea:\n",
    "    #    \"con-\\ntinente\" → \"continente\"\n",
    "    texto = re.sub(r\"-\\s*\\n\\s*\", \"\", texto)\n",
    "\n",
    "    # 3. Reparar saltos de línea dentro de frases\n",
    "    #    (unir líneas cuando la anterior no termina en . ! ?)\n",
    "    texto = re.sub(r\"(?<![.!?])\\n(?!\\n)\", \" \", texto)\n",
    "\n",
    "    # 4. Normalizar saltos entre párrafos (dejar doble salto limpio)\n",
    "    texto = re.sub(r\"\\n\\s*\\n\", \"\\n\\n\", texto)\n",
    "\n",
    "    # 5. Eliminar espacios múltiples consecutivos\n",
    "    texto = re.sub(r\" {2,}\", \" \", texto)\n",
    "\n",
    "    # 6. Eliminar espacios del principio y final\n",
    "    return texto.strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Cargamos el texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nos traemos el texto que queremos analizar:\n",
    "with open(\"../data/lolly_willowes.txt\", \"r\", encoding=\"utf-8\") as l:\n",
    "    raw_lolly = l.read()\n",
    "\n",
    "lolly = limpiar_gutenberg_completo(raw_lolly)\n",
    "\n",
    "# Lo guardamos:\n",
    "with open(\"../data/lolly_clean.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(lolly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "289977"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Ahora \"lolly\" es una cadena larguísima de texto, así que necesitamos hacer las pertinentes modificaciones\n",
    "display(type(lolly))\n",
    "display(len(lolly))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. SpaCy para limpiar, tokenizar y lematizar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos el modelo\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Stopwords personalizadas (partimos de las estándar)\n",
    "custom_stopwords = set(STOP_WORDS)\n",
    "\n",
    "# Palabras que queremos conservar aunque sean stopwords habituales\n",
    "wtk = {\n",
    "    \"not\", \"no\", \"never\",\n",
    "    \"yet\", \"still\", \"though\",\n",
    "    \"perhaps\", \"maybe\"\n",
    "}\n",
    "\n",
    "# Las retiramos de las stopwords personalizadas\n",
    "for w in wtk:\n",
    "    custom_stopwords.discard(w)\n",
    "\n",
    "# Función auxiliar (si quieres seguir usándola)\n",
    "def clean_tokens(lolly):\n",
    "    doc = nlp(lolly)\n",
    "    return [\n",
    "        token.lemma_.lower()\n",
    "        for token in doc\n",
    "        if token.is_alpha and token.lemma_.lower() not in custom_stopwords\n",
    "    ]\n",
    "\n",
    "\n",
    "# Función que devuelve el DF completo\n",
    "\n",
    "def procesar_texto_df(texto):\n",
    "    \"\"\"\n",
    "    Procesa un texto con spaCy y devuelve un DataFrame con información lingüística.\n",
    "    Aplica las stopwords personalizadas definidas en custom_stopwords.\n",
    "    \n",
    "    Parámetros\n",
    "    ----------\n",
    "    texto : str\n",
    "        Texto ya limpio.\n",
    "\n",
    "    Retorna\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        Un DataFrame con columnas:\n",
    "        - token: forma original\n",
    "        - lema: forma lematizada en minúsculas\n",
    "        - pos: categoría morfosintáctica\n",
    "        - tag: etiqueta morfológica detallada\n",
    "        - dep: tipo de dependencia sintáctica\n",
    "        - es_stop_custom: si está en tus stopwords personalizadas\n",
    "        - es_stop_spacy: si spaCy la marca como stopword\n",
    "    \"\"\"\n",
    "    doc = nlp(texto)\n",
    "\n",
    "    rows = []\n",
    "    for token in doc:\n",
    "        if not token.is_alpha:\n",
    "            continue\n",
    "\n",
    "        lema = token.lemma_.lower()\n",
    "        rows.append({\n",
    "            \"token\": token.text,\n",
    "            \"lema\": lema,\n",
    "            \"pos\": token.pos_,\n",
    "            \"tag\": token.tag_,\n",
    "            \"dep\": token.dep_,\n",
    "            \"es_stop_custom\": lema in custom_stopwords,\n",
    "            \"es_stop_spacy\": token.is_stop\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(rows)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Transformación en DF y cotilleo superficial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = clean_tokens(lolly)\n",
    "df_tokens = pd.DataFrame(tokens, columns=[\"token\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22152, 1)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9446</th>\n",
       "      <td>spot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6126</th>\n",
       "      <td>leave</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14189</th>\n",
       "      <td>path</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11327</th>\n",
       "      <td>screen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17054</th>\n",
       "      <td>demetrius</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20881</th>\n",
       "      <td>believe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1738</th>\n",
       "      <td>home</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4582</th>\n",
       "      <td>morning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22017</th>\n",
       "      <td>catch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12126</th>\n",
       "      <td>slide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6131</th>\n",
       "      <td>vicar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19065</th>\n",
       "      <td>subject</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18547</th>\n",
       "      <td>shall</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17052</th>\n",
       "      <td>great</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11718</th>\n",
       "      <td>saunter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1239</th>\n",
       "      <td>play</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7865</th>\n",
       "      <td>having</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2531</th>\n",
       "      <td>nostril</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12973</th>\n",
       "      <td>not</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9504</th>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           token\n",
       "9446        spot\n",
       "6126       leave\n",
       "14189       path\n",
       "11327     screen\n",
       "17054  demetrius\n",
       "20881    believe\n",
       "1738        home\n",
       "4582     morning\n",
       "22017      catch\n",
       "12126      slide\n",
       "6131       vicar\n",
       "19065    subject\n",
       "18547      shall\n",
       "17052      great\n",
       "11718    saunter\n",
       "1239        play\n",
       "7865      having\n",
       "2531     nostril\n",
       "12973        not\n",
       "9504          no"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Contar los tokens totales \n",
    "display(df_tokens.shape)\n",
    "# Ver ejemplos aleatorios\n",
    "display(df_tokens.sample(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    22152.000000\n",
       "mean         5.610599\n",
       "std          2.081681\n",
       "min          1.000000\n",
       "25%          4.000000\n",
       "50%          5.000000\n",
       "75%          7.000000\n",
       "max         18.000000\n",
       "Name: longitud, dtype: float64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Describir el tamaño de las palabras para ver por qué tiene preferencia la autora\n",
    "df_tokens[\"longitud\"] = df_tokens[\"token\"].str.len()\n",
    "df_tokens[\"longitud\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparativa de la longitud media de las palabras\n",
    "\n",
    "A continuación se presenta una referencia de longitudes medias de palabra típicas en distintos tipos de texto en inglés:\n",
    "\n",
    "| Tipo de texto               | Longitud media |\n",
    "|-----------------------------|----------------|\n",
    "| Inglés conversacional       | 3.5–4.5        |\n",
    "| Prensa / no ficción         | 4.5–5.0        |\n",
    "| Literatura del s. XX        | 5.0–6.0        |\n",
    "| Literatura del s. XIX       | 5.5–6.5        |\n",
    "\n",
    "**Valor obtenido para Sylvia Townsend Warner:** 5.61  \n",
    "\n",
    "---\n",
    "\n",
    "### Percentiles del texto analizado\n",
    "\n",
    "- **25% = 4 caracteres** → un cuarto de las palabras son muy cortas (and, with, when, will…).  \n",
    "- **50% = 5 caracteres** → la palabra “típica” tiene 5 letras.  \n",
    "- **75% = 7 caracteres** → solo un cuarto del vocabulario supera las 7 letras, lo cual es totalmente normal en literatura inglesa.\n",
    "\n",
    "---\n",
    "\n",
    "### Comparación con otras autoras\n",
    "\n",
    "| Autora              | Media aproximada |\n",
    "|---------------------|------------------|\n",
    "| Jane Austen         | 5.4–5.6          |\n",
    "| Emily Brontë        | 5.7–5.9          |\n",
    "| Thomas Hardy        | 5.8–6.2          |\n",
    "| Virginia Woolf      | 5.8–6.1          |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribución léxica y hapax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "token\n",
       "laura       396\n",
       "not         333\n",
       "come        199\n",
       "like        199\n",
       "no          178\n",
       "titus       137\n",
       "caroline    127\n",
       "think       127\n",
       "henry       124\n",
       "look        118\n",
       "know        117\n",
       "great       113\n",
       "feel        111\n",
       "day          97\n",
       "time         93\n",
       "little       92\n",
       "old          90\n",
       "place        85\n",
       "willowes     83\n",
       "walk         82\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Distribución léxica con frecuencias básicas\n",
    "frecuencias = df_tokens[\"token\"].value_counts()\n",
    "frecuencias.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "token\n",
       "laura       1.787649\n",
       "not         1.503250\n",
       "come        0.898339\n",
       "like        0.898339\n",
       "no          0.803539\n",
       "titus       0.618454\n",
       "caroline    0.573312\n",
       "think       0.573312\n",
       "henry       0.559769\n",
       "look        0.532683\n",
       "know        0.528169\n",
       "great       0.510112\n",
       "feel        0.501083\n",
       "day         0.437884\n",
       "time        0.419827\n",
       "little      0.415312\n",
       "old         0.406284\n",
       "place       0.383713\n",
       "willowes    0.374684\n",
       "walk        0.370170\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Distribución léxica con frecuencias normalizadas\n",
    "frecuencias_rel = df_tokens[\"token\"].value_counts(normalize=True) * 100\n",
    "frecuencias_rel.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "token\n",
       "distinguished    1\n",
       "grapple          1\n",
       "distribute       1\n",
       "mount            1\n",
       "baking           1\n",
       "inland           1\n",
       "kettle           1\n",
       "cope             1\n",
       "thinly           1\n",
       "hallowed         1\n",
       "complainingly    1\n",
       "hardness         1\n",
       "tracery          1\n",
       "celery           1\n",
       "meandering       1\n",
       "martial          1\n",
       "townsend         1\n",
       "retort           1\n",
       "linoleum         1\n",
       "hemlock          1\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "2777"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ¿Encontraremos algún hapax?\n",
    "hapax = frecuencias[frecuencias == 1]\n",
    "display(hapax.sample(20))\n",
    "display(len(hapax))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS, TAG, DEP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          token      lema    pos   tag        dep  es_stop_custom  \\\n",
      "24563       she       she   PRON   PRP      nsubj            True   \n",
      "24690  striking    strike   VERB   VBG      xcomp           False   \n",
      "32382       the       the    DET    DT        det            True   \n",
      "25833        he        he   PRON   PRP      nsubj            True   \n",
      "42936       his       his   PRON  PRP$       poss            True   \n",
      "18257        It        it   PRON   PRP      nsubj            True   \n",
      "44650        it        it   PRON   PRP      nsubj            True   \n",
      "35759       and       and  CCONJ    CC         cc            True   \n",
      "8301   sunlight  sunlight   NOUN    NN       pobj           False   \n",
      "6247     greens     green   NOUN   NNS       pobj           False   \n",
      "7670       that      that  SCONJ    IN       mark            True   \n",
      "44371     flies       fly   NOUN   NNS  nsubjpass           False   \n",
      "48646       you       you   PRON   PRP       dobj            True   \n",
      "46973       his       his   PRON  PRP$       poss            True   \n",
      "4850        had      have    AUX   VBD        aux            True   \n",
      "51883     thing     thing   NOUN    NN       dobj           False   \n",
      "36221       she       she   PRON   PRP      nsubj            True   \n",
      "44485      call      call   VERB    VB       conj            True   \n",
      "36288       and       and  CCONJ    CC         cc            True   \n",
      "35726        he        he   PRON   PRP      nsubj            True   \n",
      "\n",
      "       es_stop_spacy  \n",
      "24563           True  \n",
      "24690          False  \n",
      "32382           True  \n",
      "25833           True  \n",
      "42936           True  \n",
      "18257           True  \n",
      "44650           True  \n",
      "35759           True  \n",
      "8301           False  \n",
      "6247           False  \n",
      "7670            True  \n",
      "44371          False  \n",
      "48646           True  \n",
      "46973           True  \n",
      "4850            True  \n",
      "51883          False  \n",
      "36221           True  \n",
      "44485           True  \n",
      "36288           True  \n",
      "35726           True  \n"
     ]
    }
   ],
   "source": [
    "# Procesar el texto para sacar token, lema, pos, tag, dep y si son STOP WORDS (customizado y original)\n",
    "df = procesar_texto_df(lolly)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>lema</th>\n",
       "      <th>pos</th>\n",
       "      <th>tag</th>\n",
       "      <th>dep</th>\n",
       "      <th>es_stop_custom</th>\n",
       "      <th>es_stop_spacy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>26248</th>\n",
       "      <td>She</td>\n",
       "      <td>she</td>\n",
       "      <td>PRON</td>\n",
       "      <td>PRP</td>\n",
       "      <td>nsubj</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13764</th>\n",
       "      <td>housekeeping</td>\n",
       "      <td>housekeeping</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NN</td>\n",
       "      <td>pobj</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>she</td>\n",
       "      <td>she</td>\n",
       "      <td>PRON</td>\n",
       "      <td>PRP</td>\n",
       "      <td>nsubj</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44196</th>\n",
       "      <td>Lost</td>\n",
       "      <td>lost</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>NNP</td>\n",
       "      <td>npadvmod</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25432</th>\n",
       "      <td>well</td>\n",
       "      <td>well</td>\n",
       "      <td>ADV</td>\n",
       "      <td>RB</td>\n",
       "      <td>advmod</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4474</th>\n",
       "      <td>and</td>\n",
       "      <td>and</td>\n",
       "      <td>CCONJ</td>\n",
       "      <td>CC</td>\n",
       "      <td>cc</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32698</th>\n",
       "      <td>comfortable</td>\n",
       "      <td>comfortable</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>JJ</td>\n",
       "      <td>amod</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46929</th>\n",
       "      <td>very</td>\n",
       "      <td>very</td>\n",
       "      <td>ADV</td>\n",
       "      <td>RB</td>\n",
       "      <td>advmod</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16574</th>\n",
       "      <td>this</td>\n",
       "      <td>this</td>\n",
       "      <td>DET</td>\n",
       "      <td>DT</td>\n",
       "      <td>det</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3553</th>\n",
       "      <td>for</td>\n",
       "      <td>for</td>\n",
       "      <td>ADP</td>\n",
       "      <td>IN</td>\n",
       "      <td>prep</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14941</th>\n",
       "      <td>mastered</td>\n",
       "      <td>master</td>\n",
       "      <td>VERB</td>\n",
       "      <td>VBN</td>\n",
       "      <td>relcl</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44218</th>\n",
       "      <td>of</td>\n",
       "      <td>of</td>\n",
       "      <td>ADP</td>\n",
       "      <td>IN</td>\n",
       "      <td>prep</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12756</th>\n",
       "      <td>become</td>\n",
       "      <td>become</td>\n",
       "      <td>VERB</td>\n",
       "      <td>VBN</td>\n",
       "      <td>xcomp</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12856</th>\n",
       "      <td>that</td>\n",
       "      <td>that</td>\n",
       "      <td>SCONJ</td>\n",
       "      <td>IN</td>\n",
       "      <td>mark</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3475</th>\n",
       "      <td>winter</td>\n",
       "      <td>winter</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NN</td>\n",
       "      <td>pobj</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36931</th>\n",
       "      <td>was</td>\n",
       "      <td>be</td>\n",
       "      <td>AUX</td>\n",
       "      <td>VBD</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51075</th>\n",
       "      <td>at</td>\n",
       "      <td>at</td>\n",
       "      <td>ADP</td>\n",
       "      <td>IN</td>\n",
       "      <td>prep</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36437</th>\n",
       "      <td>surprise</td>\n",
       "      <td>surprise</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NN</td>\n",
       "      <td>nsubj</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25110</th>\n",
       "      <td>have</td>\n",
       "      <td>have</td>\n",
       "      <td>VERB</td>\n",
       "      <td>VB</td>\n",
       "      <td>conj</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24021</th>\n",
       "      <td>care</td>\n",
       "      <td>care</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NN</td>\n",
       "      <td>pobj</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              token          lema    pos  tag       dep  es_stop_custom  \\\n",
       "26248           She           she   PRON  PRP     nsubj            True   \n",
       "13764  housekeeping  housekeeping   NOUN   NN      pobj           False   \n",
       "218             she           she   PRON  PRP     nsubj            True   \n",
       "44196          Lost          lost  PROPN  NNP  npadvmod           False   \n",
       "25432          well          well    ADV   RB    advmod            True   \n",
       "4474            and           and  CCONJ   CC        cc            True   \n",
       "32698   comfortable   comfortable    ADJ   JJ      amod           False   \n",
       "46929          very          very    ADV   RB    advmod            True   \n",
       "16574          this          this    DET   DT       det            True   \n",
       "3553            for           for    ADP   IN      prep            True   \n",
       "14941      mastered        master   VERB  VBN     relcl           False   \n",
       "44218            of            of    ADP   IN      prep            True   \n",
       "12756        become        become   VERB  VBN     xcomp            True   \n",
       "12856          that          that  SCONJ   IN      mark            True   \n",
       "3475         winter        winter   NOUN   NN      pobj           False   \n",
       "36931           was            be    AUX  VBD      ROOT            True   \n",
       "51075            at            at    ADP   IN      prep            True   \n",
       "36437      surprise      surprise   NOUN   NN     nsubj           False   \n",
       "25110          have          have   VERB   VB      conj            True   \n",
       "24021          care          care   NOUN   NN      pobj           False   \n",
       "\n",
       "       es_stop_spacy  \n",
       "26248           True  \n",
       "13764          False  \n",
       "218             True  \n",
       "44196          False  \n",
       "25432           True  \n",
       "4474            True  \n",
       "32698          False  \n",
       "46929           True  \n",
       "16574           True  \n",
       "3553            True  \n",
       "14941          False  \n",
       "44218           True  \n",
       "12756           True  \n",
       "12856           True  \n",
       "3475           False  \n",
       "36931           True  \n",
       "51075           True  \n",
       "36437          False  \n",
       "25110           True  \n",
       "24021          False  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Ver resultado general\n",
    "display(df.sample(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pos\n",
       "NOUN     8975\n",
       "VERB     7244\n",
       "PRON     6716\n",
       "ADP      6014\n",
       "DET      4885\n",
       "AUX      3630\n",
       "ADJ      3527\n",
       "ADV      3125\n",
       "PROPN    2621\n",
       "CCONJ    2321\n",
       "SCONJ    1493\n",
       "PART     1201\n",
       "NUM       239\n",
       "INTJ       63\n",
       "X           4\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ver la frecuencia de utilización de las diferentes categorías gramaticales\n",
    "\n",
    "df[\"pos\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pos\n",
       "NOUN     17.240386\n",
       "VERB     13.915248\n",
       "PRON     12.900995\n",
       "ADP      11.552499\n",
       "DET       9.383764\n",
       "AUX       6.972992\n",
       "ADJ       6.775135\n",
       "ADV       6.002920\n",
       "PROPN     5.034769\n",
       "CCONJ     4.458489\n",
       "SCONJ     2.867955\n",
       "PART      2.307042\n",
       "NUM       0.459103\n",
       "INTJ      0.121019\n",
       "X         0.007684\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ver la frecuencia normalizada de utilización de las diferentes categorías gramaticales\n",
    "\n",
    "df[\"pos\"].value_counts(normalize=True) * 100"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
